\documentclass{article}

\usepackage{amsmath, mathrsfs, amssymb, stmaryrd, cancel, hyperref, relsize,tikz,amsthm}
\usepackage{graphicx}
\usepackage{xfrac}
\hypersetup{pdfstartview={XYZ null null 1.25}}
\usepackage[all]{xy}
\usepackage[normalem]{ulem}
\usepackage{tikz-cd}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]{\bfseries}{\itshape}
\newtheorem{proposition}[theorem]{Proposition}{\bfseries}{\itshape}
\newtheorem{definition}[theorem]{Definition}{\bfseries}{\upshape}
\newtheorem{lemma}[theorem]{Lemma}{\bfseries}{\upshape}
\newtheorem{example}[theorem]{Example}{\bfseries}{\upshape}
\newtheorem{corollary}[theorem]{Corollary}{\bfseries}{\upshape}
\newtheorem{remark}[theorem]{Remark}{\bfseries}{\upshape}
\newtheorem{fact}[theorem]{Fact}{\bfseries}{\upshape}
\newtheorem{Q}[theorem]{Exercise}{\bfseries}{\upshape}

\newtheorem*{theorem*}{Theorem}

\newcommand{\bN}{\mathbb{N}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bC}{\mathbb{C}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\spa}{\mathrm{span}}

\title{ITCS 531 \\Linear Algebra 2: Dimension}
\author{Rob Egrot}
\date{}

\begin{document}
\maketitle
\section{Dimension}
\paragraph{Bases}
Our mental model for vector spaces should be something like $\bR^2$, the Euclidean plane. In $\bR^2$, every vector is defined by coordinates $(x,y)$. In other words, every vector in $\bR^2$ can be written as a sum $x(1,0)+y(0,1)$ of the vectors $(1,0)$ and $(0,1)$. These vectors $(1,0)$ and $(0,1)$ are very special, as their linear combinations generate the whole of $\bR^2$, and any set of vectors with this property must have at least size two. The following definition generalizes this idea to abstract vector spaces.

\begin{definition}
If $V$ is a vector space, then a \emph{basis} for $V$ is a linearly independent set that spans $V$.
\end{definition}

\begin{lemma}
Let $V$ be a vector space over $\bF$. Let $v_1,\ldots,v_n\in V$. Then $(v_1,\ldots,v_n)$ is a basis for $V$ if and only if every element $u$ can be expressed as $a_1v_1+\ldots + a_nv_n$, for some unique $\{a_1,\ldots,a_n\}\subseteq \bF$. 
\end{lemma}
\begin{proof}
Suppose $(v_1,\ldots,v_n)$ is a basis for $V$. Then, given $u\in V$, that $u= a_1v_1+\ldots + a_nv_n$ for some $\{a_1,\ldots,a_n\}\subseteq \bF$ follows directly from the fact that $(v_1,\ldots,v_n)$ spans $V$. Moreover, if $u = a_1v_1+\ldots + a_nv_n = b_1v_1+\ldots + b_nv_n$, then $0 = (a_1-b_1)v_1+\ldots +(a_n-b_n)v_n$, so $a_i=b_i$ for all $i\in \{1,\ldots,n\}$, as $(v_1,\ldots,v_n)$ is linearly independent.

Conversely, if $(v_1,\ldots,v_n)$ satisfies the two stated properties then it is a linearly independent spanning set. To see this note that it obviously spans $V$, and it is linearly independent as the only way to express $0$ as $a_1v_1+\ldots+ a_nv_n$ is with $a_1=\ldots=a_n$. This proves the result.  
\end{proof}

Bases are extremely important in the study of vector spaces because, like the prime numbers generate the integers, a vector space is generated by a basis. In other words, if you have a basis, then you know the space. There are natural questions we can ask about bases. Does every vector space have one? Can a space have more than one? If a space has two (or more) bases, are they essentially equivalent? In other words, does it matter what basis we choose when working with a vector space? We will see answers to these questions soon, but first we need the following useful technical lemma.

\begin{lemma}\label{L:LA2tech}
Let $V$ be a vector space over $\bF$, and let $v_1,\ldots v_n\in V$. Suppose that $(v_1,\ldots,v_n)$ is linearly dependent. Then there is $j\in\{1,\ldots,n\}$ such that:
\begin{enumerate}
\item $v_j \in \spa(v_1,\ldots,v_{j-1})$.
\item $\spa(v_1,\ldots , v_{j-1}, v_{j+1},\ldots, v_n) = \spa(v_1,\ldots,v_n)$.
\end{enumerate} 
\end{lemma}
\begin{proof}
Since $(v_1,\ldots,v_n)$ is linearly dependent, by the definition of linear dependence there are $a_1,\ldots,a_n\in \bF$ such that $a_1v_1+\ldots + a_nv_n = 0$ and such that at least some $a_i$ is not equal to zero. Let $j$ be the largest value such that $a_j\neq 0$. Then $a_1v_1 + \ldots +a_j v_j = 0$, and, since $a_j\neq 0$ we can rewrite this as $v_j = -\frac{a_1}{a_j}v_1-\ldots -\frac{a_{j-1}}{a_j}v_{j-1}$. This proves (1), and (2) follows easily from (1), so we are done. 
\end{proof}

\begin{proposition}\label{P:LA2length}
Let $V$ be a vector space over $\bF$, let $(u_1,\ldots,u_k)$ be linearly independent, and let $(v_1,\ldots,v_n)$ span $V$. Then $k\leq n$. In other words, linearly independent lists of vectors cannot be bigger than spanning lists of vectors. 
\end{proposition}
\begin{proof}
We will use lemma \ref{L:LA2tech} multiple times.


The idea behind the proof is to replace elements of $(v_1,\ldots,v_n)$ with different elements of $(u_1,\ldots,u_k)$, till we have used all the elements of $(u_1,\ldots,u_k)$. Being able to do this implies that $k\leq n$. 

To do this, for each $i\in\{1,\ldots,k\}$, we will define a list $s_i$ such that the following properties are satisfied:
\begin{enumerate}
\item For all $j\in \{1,\ldots,k-1\}$, if $s_j = (u_j,u_{j-1},\ldots, u_1,v'_1,\ldots, v'_{n-j})$, then the list $s_{j+1}$ has form 
\[(u_{j+1},u_j,u_{j-1},\ldots, u_1,v''_1,\ldots, v''_{n-(j+1)}),\] 
where $\{v''_1,\ldots, v''_{n-(j+1)}\}\subset \{v'_1,\ldots, v'_{n-j}\}$. 
\item $\spa(s_j) = V$ for all $j\in \{1,\ldots,k\}$.
\end{enumerate}

Consider the list $(u_1,v_1,\ldots,v_n)$. By lemma \ref{L:LA2tech} there is an element $w$ of $(u_1,v_1,\ldots,v_n)$ such that $w$ is in the span of the part of the list $(u_1,v_1,\ldots,v_n)$ that precedes it. Obviously we can't have $w = u_1$, as then $u_1$ would have to be $0$, as this is the only thing in the span of the empty list. So $w$ is in $(v_1,\ldots,v_n)$, and we define $s_1$ by removing $w$ from $(u_1,v_1,\ldots,v_n)$. We proceed using a recursive process. Suppose we have constructed the lists 
\[s_1,\ldots,s_i = (u_i,u_{i-1},\ldots, u_1,v'_1,\ldots, v'_{n-i})\] 
with the required properties. Then, as $s_i$ spans $V$, the list 
\[(u_{i+1},u_i,u_{i-1},\ldots, u_1,v'_1,\ldots, v'_{n-i})\]
must be linearly dependent. So, again by lemma \ref{L:LA2tech}, there is an element $w$ of $(u_{i+1},u_i,u_{i-1},\ldots, u_1,v'_1,\ldots, v'_{n-i})$ such that $w$ is in the span of the part of the list $(u_{i+1},u_i,u_{i-1},\ldots, u_1,v'_1,\ldots, v'_{n-i})$ that precedes it, and the span of the list obtained by removing $w$ is still $V$. 

Since $(u_1,\ldots,u_k)$ is linearly independent, $w$ cannot be a member of $\{u_{i+1},\ldots, u_1\}$, so the list we get by removing $w$ has form 
\[(u_{i+1},u_i,u_{i-1},\ldots, u_1,v''_1,\ldots, v''_{n-(i+1)}),\] 
where $\{v''_1,\ldots, v''_{n-(i+1)}\}\subset \{v'_1,\ldots, v'_{n-i}\}$. We define $s_{i+1}$ to be this new list, noting that it satisfies the required properties.

This construction works until we hit the limit $i=k$. This proves the result, because every time we remove an element it must be a new element from the original list $(v_1,\ldots,v_n)$. Since we remove $k$ elements in total, this tells us that $n$ cannot be smaller than $k$. I.e. $k\leq n$ as required.  
\end{proof}


\paragraph{Defining dimension}

\begin{definition}
A vector space $V$ is \emph{finite dimensional} if it contains a finite spanning list $(v_1,\ldots,v_n)$. If $V$ is not finite dimensional then it is \emph{infinite dimensional}. 
\end{definition}

\begin{theorem}\label{T:LA2basis}
Let $V$ be a vector space over $\bR$. Then:
\begin{enumerate}
\item If $s = (v_1,\ldots,v_n)$ spans $V$, then $s$ can be reduced to a basis for $V$.
\item If $V$ is finite dimensional, and if $t=(u_1,\ldots,u_k)$ is linearly independent in $V$, then $t$ can be extended to a basis for $V$.
\end{enumerate}
\end{theorem}
\begin{proof}
For (1), we define a new list $s'$ as follows. First, if $v_1=0$, then we remove $v_1$.  Then, for every $i\in \{2,\ldots, n\}$, if $v_i\in \spa(v_1,\ldots,v_{i-1})$, we remove $v_i$. Now, $s'$ still spans $V$, because we only removed elements in the span of the preceding elements in the list. Also, $s'$ is linearly independent, because if it were not it would be possible to remove an element in the span of preceding elements (by lemma \ref{L:LA2tech}). Since we removed all these elements, this is not possible, so $s'$ must be linearly independent. Thus $s'$ is a basis for $V$.

For (2), since $V$ is finite dimensional it has a spanning list $(w_1,\ldots,w_m)$. Now, the list $(u_1,\ldots,u_k,w_1,\ldots,w_m)$ also spans $V$, and so, by (1), reduces to a basis for $V$. The process defined in the proof of (1) does not remove any elements of $t$, as $t$ is linearly independent, so the resulting list extends $t$ as required. 
\end{proof}

\begin{corollary}\label{C:basis}
Every finite dimensional vector space has a basis.
\end{corollary}
\begin{proof}
Just reduce the finite spanning list to a basis. 
\end{proof}

It is also true (in classical mathematics), that every \emph{infinite} dimensional vector space also has a basis, but this proof is more difficult, and involves using an infinite choice principle. 

\begin{proposition}\label{P:LA2basis}
If $V$ is a finite vector space then every basis for $V$ has the same length.
\end{proposition}
\begin{proof}
Let $s$ and $t$ be bases for $V$. Then, as $s$ is linearly independent and $t$ spans $V$, by proposition \ref{P:LA2length}, we must have $|s|\leq |t|$. But $t$ is also linearly independent, and $s$ also spans $V$, so by the same proposition we also have $|t|\leq |s|$. So $|s|=|t|$ as claimed.
\end{proof}

In view of corollary \ref{C:basis} and proposition \ref{P:LA2basis}, we can define the basis of a finite dimensional vector space in terms of the size of it's possible bases.
\begin{definition}
If $V$ is a finite dimensional vector space, then we define the \emph{dimension} of $V$ to be the size of its bases. We use $\dim(V)$ to denote the dimension of $V$.
\end{definition}

\begin{example}\mbox{}
\begin{enumerate}
\item The vectors $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$ provide a basis for $\bR^3$. So $\dim(\bR^3) = 3$.
\item The vectors $(2,0,1)$, $(2,3,0)$ and $(0,6,-1)$ also provide a basis for $\bR^3$.
\item The vectors $(1,2,3)$, $(-1,-1,0)$, $(1,1,1)$ and $(3,-2,0)$ must be linearly dependent in $\bR^3$, because $\dim(\bR^3) = 3$, and so every linearly independent list must have length at most 3 (by proposition \ref{P:LA2length}). 
\item The vectors, $1,x,x^2,x^4,\ldots$ provide a basis for $\bR[x]$. As no finite list spans $\bR[x]$, it follows that $\bR[x]$ is infinite dimensional.  
\end{enumerate}
\end{example}

It turns out that in a finite dimensional vector space, lists that are spanning/linearly independent must be bases if they are the right size.
\begin{theorem}\label{T:span_ind}
Let $V$ be a finite dimensional vector space. Then:
\begin{enumerate}
\item If $s$ is a spanning list for $V$ and $|s|=\dim(V)$ then $s$ is a basis for $V$.
\item If $t$ is a linearly independent list in $V$ and $|t|=\dim(V)$ then $t$ is a basis for $V$. 
\end{enumerate}
\end{theorem}
\begin{proof}
For (1), if $s$ spans $V$ then, by theorem \ref{T:LA2basis}, $s$ can be reduced to a basis, $s'$, for $V$. By proposition \ref{P:LA2basis} we must have $|s'| = \dim(V) = |s|$, so $s'$ must be equal to $s$, and thus $s$ is a basis for $V$ as required.

For (2), if $t$ is linearly independent, then, again by theorem \ref{T:LA2basis}, $t$ can be extended to a basis, $t'$. for $V$. As in part (1) we have $|t'|=\dim(V)=|t|$, so $t$ is indeed a basis for $V$.   
\end{proof}

\paragraph{Dimension and subspaces}
As we would expect, subspaces of finite dimensional vector spaces are themselves finite dimensional. In fact, their dimension can be at most as large as the dimension of the original space. Which is also what we would expect. 
\begin{proposition}\label{P:LA2subbase}
Every subspace of a finite dimensional vector space is finite dimensional.
\end{proposition}
\begin{proof}
Let $V$ be a finite dimensional vector space and let $U$ be a subspace of $V$. If $U=\{0\}$ then the empty list spans $U$, so there is nothing to do. If $U\neq \{0\}$ then we construct a basis for $U$ by recursion as follows:
\begin{itemize}
\item Since $U\neq\{0\}$ we can choose $v_1 \in U\setminus\{0\}$. Define $s_1 = (v_1).$
\item Given linearly independent $s_i = (v_1,\ldots, v_i)$ in $U$, if $s_i$ does not span $U$ then there is $v_{i+1}\in U\setminus \spa(s_i)$. In this case define $s_{i+1} = (v_1,\ldots, v_i, v_{i+1})$.  
\end{itemize} 
Clearly $s_i$ is linearly independent for all $i$. Moreover, $s_i$ can be no longer than the dimension of $U$ (by proposition \ref{P:LA2length}), so at some point the process must terminate. That is, there is $k$ such that $U=\spa(S_k)$, and then $s_k$ is a finite basis for $U$ as required.  
\end{proof}

\begin{corollary}
If $V$ is a finite dimensional vector space and $U$ is a subspace of $V$, then $\dim(U)\leq \dim(V)$.
\end{corollary}
\begin{proof}
Let $t= (v_1,\ldots,v_n)$ be a basis for $V$, and, using proposition \ref{P:LA2subbase}, let $s= (u_1,\ldots,u_k)$ be a basis for $U$. Then $s$ is linearly independent in $V$, and $t$ spans $V$, so $|s|\leq |t|$ by proposition \ref{P:LA2length}. Thus $\dim(U)\leq \dim(V)$ as claimed.
\end{proof}

Every non-trivial subspace of a vector space can be `extended' via a direct sum to the whole space.
\begin{proposition}
Let $V$ be a finite dimensional vector space, and let $U$ be a subspace of $V$. Then there is a subspace $W$ of $V$ such that $V=U\oplus W$.
\end{proposition}
\begin{proof}
Let $s=(u_1,\ldots,u_k)$ be a basis for $U$. Then $s$ is linearly independent in $V$, so, by theorem \ref{T:LA2basis}, $s$ can be extended to a basis $(u_1,\ldots,u_k,w_1,\ldots,w_m)$ for $V$. We define $W$ to be $\spa(w_1,\ldots,w_m)$.

To see that $V = U\oplus W$ we just have to check that $V = U + W$, and $U\cap W =\{0\}$ (using lemma \ref{L:LA1cap}). Now, that $V = U + W$ follows immediately from the fact that $(u_1,\ldots,u_k,w_1,\ldots,w_m)$ spans $V$. That $U\cap W =\{0\}$ follows from the fact that $(u_1,\ldots,u_k)$ is basis for $U$, $(w_1,\ldots,w_m)$ is a basis for $W$, and $(u_1,\ldots,u_k,w_1,\ldots,w_m)$ is linearly independent. To see this, suppose $v\in U\cap W$. Then $v = a_1u_1+\ldots + a_ku_k$ and $v = b_1w_1+\ldots + b_mw_m$. So $0 = a_1u_1+\ldots + a_ku_k - b_1w_1-\ldots - b_mw_m$, and so $a_1=\ldots = a_k=b_1=\ldots = b_m = 0$, by linear independence of $(u_1,\ldots,u_k,w_1,\ldots,w_m)$. Thus $v = 0$.  
\end{proof}

The next result is a bit like the inclusion-exclusion principal for counting the size of the union of two finite sets.

\begin{proposition}
Let $V$ be a finite dimensional vector space, and let $U$ and $W$ be subspaces of $V$. Then $\dim(U+W)= \dim(U) +\dim(W) - \dim(U\cap W)$.
\end{proposition}
\begin{proof}
Let $(v_1,\ldots,v_n)$ be a basis for $U\cap W$ (which is a  subspace of $V$ by exercise 1.6). By theorem \ref{T:LA2basis}, we can extend $(v_1,\ldots,v_n)$ to a basis $(u_1,\ldots,u_k,v_1,\ldots,v_n)$ for $U$, and a basis $(v_1,\ldots,v_n, w_1,\ldots, w_m)$ for $W$. We claim that 
\[s=(u_1,\ldots,u_k,v_1,\ldots,v_n, w_1,\ldots, w_m)\] 
is a basis for $U+W$. To see that this is true, note first that $s$ clearly spans $U+W$, so it remains only to show that it is linearly independent.

So, suppose that 
\[a_1u_1+\ldots + a_k u_k + b_1v_1+\ldots+ b_nv_n+ c_1w_1+\ldots+ c_mw_m = 0.\]
Then 
\[c_1w_1+\ldots+ c_mw_m = -a_1u_1-\ldots - a_k u_k - b_1v_1-\ldots- b_nv_n,\]
and it follows that $c_1w_1+\ldots+ c_mw_m\in U\cap W$, so there are $b'_1,\ldots b'_n\in\bF$ such that $c_1w_1+\ldots+ c_mw_m = b'_1v_1+\ldots+ b'_nv_n$. In other words,
\[c_1w_1+\ldots+ c_mw_m - b'_1v_1-\ldots- b'_nv_n = 0.\]
But $(v_1,\ldots,v_n, w_1,\ldots, w_m)$ is a basis for $W$, and so is linearly independent, and so it follows that $c_i = 0$ for all $i\in \{1,\ldots,m\}$. But then we have 
\[a_1u_1+\ldots + a_k u_k + b_1v_1+\ldots+ b_nv_n = 0,\]
and so it follows from the fact that $(u_1,\ldots,u_k,v_1,\ldots,v_n)$ is a basis for $U$ that all the coefficients in this expression are also 0. So $s$ is linearly independent as required.      
\end{proof}

\end{document}